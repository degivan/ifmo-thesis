{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считываем твиты, выделяем фичи\n",
    "\n",
    "1. Достаем чистые твиты\n",
    "2. Достаем development твиты\n",
    "3. Достаем грязные твиты\n",
    "4. Получаем фичи\n",
    "5. Сохраняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import path, listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "import resource\n",
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GaussianNoise, SimpleRNN, LSTM, Reshape, Embedding, SpatialDropout1D, GaussianDropout, Conv1D, GlobalMaxPooling1D, Flatten, MaxPooling1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from collections import namedtuple\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import random as rn\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 1\n",
    "else:\n",
    "    num_CPU = 1\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4,\\\n",
    "        inter_op_parallelism_threads=4, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_from_emotion = {}\n",
    "position_from_emotion['anger'] = [1, 0, 0, 0]\n",
    "position_from_emotion['sadness'] = [0, 1, 0, 0]\n",
    "position_from_emotion['joy'] = [0, 0, 1, 0]\n",
    "position_from_emotion['fear'] = [0, 0, 0, 1]\n",
    "\n",
    "cl_from_emotion = {}\n",
    "cl_from_emotion['anger'] = 0\n",
    "cl_from_emotion['sadness'] = 1\n",
    "cl_from_emotion['joy'] = 2\n",
    "cl_from_emotion['fear'] = 3\n",
    "\n",
    "\n",
    "class Tweet(object):\n",
    "    def __init__(self, message, res, common_class):\n",
    "        self.cl = cl_from_emotion[common_class]\n",
    "        self.message = message\n",
    "        self.res = [x * res for x in position_from_emotion[common_class]]\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.message) + \" \" + str(self.res)\n",
    "\n",
    "\n",
    "def get_tweet(str_tweet, res_acc=1):\n",
    "    num, message, common_class, res = str_tweet.split('\\t')\n",
    "    if res == 'NONE':\n",
    "        res = '1.000'\n",
    "    return Tweet(message, float(res[0:res_acc]), common_class)\n",
    "        \n",
    "\n",
    "\n",
    "def get_tweets(str_tweets, res_acc=1):\n",
    "    return [get_tweet(line, res_acc) for line in str_tweets.split('\\n') if len(line) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_f(y_true, y_pred):\n",
    "    fsp = y_pred - K.mean(y_pred,axis=-1,keepdims=True)\n",
    "    fst = y_true - K.mean(y_true,axis=-1, keepdims=True)\n",
    "\n",
    "    devP = K.std(y_pred)\n",
    "    devT = K.std(y_true)\n",
    "    \n",
    "    val = K.mean(fsp*fst)/(devP*devT)\n",
    "    \n",
    "    return 1 - val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_SENT140 = False # True только если есть потребность поменять что-то \n",
    "\n",
    "def format_line(id, text, em):\n",
    "    return (str(id) + '\\t' + text.replace('\\t', ' ') + '\\t' + em + '\\t1.000\\n')\n",
    "\n",
    "sent_to_emotion = {0: 'sadness', 2: 'no_emotion', 4: 'joy'}\n",
    "\n",
    "if PARSE_SENT140:\n",
    "    data = pd.read_csv('dirty_data/unlabeled/tweet_corpus.csv', encoding = 'ISO-8859-1', index_col=0, parse_dates=True,\n",
    "                      names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "    i  = 0\n",
    "    file = None\n",
    "    for id, row in data.iterrows():\n",
    "        if i % (data.shape[0] // 10) == 0:\n",
    "            part_number = str(i // (data.shape[0] // 10))\n",
    "            file = open('dirty_data/labeled/sent140part' + part_number, 'w+')\n",
    "        emotion = sent_to_emotion[id]\n",
    "        if emotion != 'no_emotion':\n",
    "            line = format_line(row[0], row[4], emotion)\n",
    "            file.write(line)\n",
    "        i += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = ['anger', 'joy', 'sadness', 'fear']\n",
    "\n",
    "def run_competition_files(path_pattern):\n",
    "    em_tweets = {}\n",
    "    for emotion in EMOTIONS:\n",
    "        filename = path.join(path_pattern % emotion)\n",
    "        file = open(filename, 'r')\n",
    "        em_tweets[emotion] = get_tweets(file.read(), res_acc=5)\n",
    "        file.close()\n",
    "    return em_tweets\n",
    "    \n",
    "train_tweets = run_competition_files('train_data/EI-reg-en_%s_train.txt')\n",
    "test_tweets = run_competition_files('development_data/2018-EI-reg-En-%s-dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_tweets =[]\n",
    "\n",
    "directory = path.join('dirty_data/labeled')\n",
    "for filename in os.listdir(directory):\n",
    "    file = open(path.join(directory,filename), 'r')\n",
    "    dirty_tweets += get_tweets(file.read(), res_acc=5)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION = 'joy'\n",
    "\n",
    "tweets = np.array(list(dirty_tweets) + list(train_tweets[EMOTION]) + list(test_tweets[EMOTION]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "    \n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def normalize_text(text):\n",
    "    stripped = re.sub(combined_pat, '', text)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_TEXT = False\n",
    "\n",
    "dirty_texts, train_texts, test_texts, texts = [], [], [], []\n",
    "if not LOAD_TEXT:\n",
    "    texts = [normalize_text(t.message) for t in tweets]\n",
    "    dirty_texts = texts[0:len(dirty_tweets)]\n",
    "    train_texts = texts[len(dirty_tweets): len(dirty_tweets) + len(train_tweets[EMOTION])]\n",
    "    test_texts = texts[len(dirty_tweets) + len(train_tweets[EMOTION]):]\n",
    "    \n",
    "    assert (len(train_texts) == len(train_tweets[EMOTION]))\n",
    "    assert (len(test_texts) == len(test_tweets[EMOTION]))\n",
    "else:\n",
    "    dirty_texts = list(np.loadtxt('features/dirty_texts.txt', dtype='str', delimiter='\\n'))\n",
    "    train_texts = list(np.loadtxt('features/train_texts_%s_.txt' % EMOTION, dtype='str', delimiter='\\n'))\n",
    "    test_texts = list(np.loadtxt('features/test_texts_%s_.txt' % EMOTION, dtype='str', delimiter='\\n'))\n",
    "    texts = dirty_texts + train_texts + test_texts\n",
    "    \n",
    "    assert (len(train_texts) == len(train_tweets[EMOTION]))\n",
    "    assert (len(test_texts) == len(test_tweets[EMOTION]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = len(dirty_tweets)\n",
    "f = s + len(train_tweets[EMOTION])\n",
    "e = f + len(test_tweets[EMOTION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_D2V = False\n",
    "\n",
    "Xd2v = {}\n",
    "\n",
    "if not LOAD_D2V:\n",
    "    model = Doc2Vec.load('doc2vec/doc2vec_model.doc2vec')\n",
    "    named_texts = [('dirty', dirty_texts), ('train', train_texts), ('test', test_texts)]\n",
    "    for name, text_list in named_texts:\n",
    "        Xd2v[name] = np.array([(model.infer_vector(x.split())) for x in text_list])\n",
    "        Xd2v[name] = Xd2v[name] - np.amin(Xd2v[name])\n",
    "        Xd2v[name] = Xd2v[name] / (np.linalg.norm(Xd2v[name]))\n",
    "    np.savetxt('features/dirty_Xd2v.txt', Xd2v['dirty']) \n",
    "    np.savetxt('features/train_Xd2v_%s.txt' % EMOTION, Xd2v['train'])\n",
    "    np.savetxt('features/dirty_Xd2v_%s.txt' % EMOTION, Xd2v['test'])\n",
    "else:\n",
    "    Xd2v['dirty'] = np.loadtxt('features/dirty_Xd2v.txt')\n",
    "    Xd2v['train'] = np.loadtxt('features/train_Xd2v_%s.txt' % EMOTION)\n",
    "    Xd2v['test'] = np.loadtxt('features/dirty_Xd2v_%s.txt' % EMOTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.setrlimit(resource.RLIMIT_CORE, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0005, max_features=2048)\n",
    "\n",
    "Xcv = vectorizer.fit_transform(texts).toarray()\n",
    "vectorizer = None\n",
    "\n",
    "texts = texts[0:old_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Xd2v = np.vstack((Xd2v['dirty'],Xd2v['train'],Xd2v['test']))\n",
    "\n",
    "batch_size = len(dirty_tweets)\n",
    "\n",
    "dirty_X = np.hstack((Xcv[0:len(dirty_tweets)], all_Xd2v[0:len(dirty_tweets)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.hstack((Xcv[s:f], all_Xd2v[s:f]))\n",
    "test_X = np.hstack((Xcv[f:e], all_Xd2v[f:e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(train_X) == len(train_tweets[EMOTION]))\n",
    "assert (len(test_X) == len(test_tweets[EMOTION]))\n",
    "\n",
    "Xcv = None\n",
    "Xd2v = None\n",
    "np.savetxt('features/train_X_%s_.txt' % EMOTION, train_X)\n",
    "np.savetxt('features/test_X_%s_.txt' % EMOTION, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "dirty_X = X[0:s]\n",
    "train_X = X[s:f]\n",
    "test_X = X[f:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([t.res for t in tweets])\n",
    "\n",
    "dirty_Y = Y[0:s]\n",
    "train_Y = Y[s:f]\n",
    "test_Y = Y[f:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_Y = np.hstack((np.ones((test_X.shape[0], 1)), np.zeros((test_X.shape[0], 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = namedtuple('Params', 'layers loss optimizer dirty_e dirty_bs train_e train_bs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_params(dirty_e, dirty_bs, train_e, train_bs, layers, optimizer='adam'):\n",
    "    return Params(layers, pearson_correlation_f, optimizer, dirty_e, dirty_bs, train_e, train_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    nm = Sequential()\n",
    "    for layer in params.layers:\n",
    "        nm.add(layer())\n",
    "    nm.compile(loss='mean_squared_error', optimizer=params.optimizer)\n",
    "    return nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 300\n",
    "embeddings = dict()\n",
    "embeddings = KeyedVectors.load_word2vec_format( \"twitter_sgns_subset.txt.gz\" , binary=False ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((max_features , embeddings_dim ) )\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if index < max_features:\n",
    "        try: embedding_weights[index,:] = embeddings[word]\n",
    "        except: embedding_weights[index,:] = np.random.rand( 1 , embeddings_dim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(27)\n",
    "\n",
    "params_list = []\n",
    "\n",
    "perceptron_layers = []\n",
    "perceptron_layers.append(lambda: Dense(1024, input_dim=train_X.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "perceptron_layers.append(lambda: Dense(256, kernel_initializer='uniform', activation='relu'))\n",
    "perceptron_layers.append(lambda: Dense(32, kernel_initializer='uniform', activation='relu'))\n",
    "perceptron_layers.append(lambda: Dense(8, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "lstm_layers = []\n",
    "lstm_layers.append(lambda: Embedding(max_features, embeddings_dim, input_length = 52, weights=[embedding_weights]))\n",
    "lstm_layers.append(lambda: Dropout(0.5))\n",
    "lstm_layers.append(lambda: Conv1D(embeddings_dim, 3, activation='relu', padding='valid', strides=1))\n",
    "lstm_layers.append(lambda: MaxPooling1D(pool_size=2))\n",
    "lstm_layers.append(lambda: LSTM(embeddings_dim, dropout=0.5, recurrent_dropout=0.5))\n",
    "lstm_layers.append(lambda: Dense(4, activation='sigmoid'))\n",
    "\n",
    "params_list.append(create_params(15, 5000, 20, 16, lstm_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " - 76s - loss: 0.0862\n",
      "Epoch 2/15\n",
      " - 72s - loss: 0.0730\n",
      "Epoch 3/15\n",
      " - 72s - loss: 0.0694\n",
      "Epoch 4/15\n",
      " - 73s - loss: 0.0672\n",
      "Epoch 5/15\n",
      " - 73s - loss: 0.0657\n",
      "Epoch 6/15\n",
      " - 72s - loss: 0.0645\n",
      "Epoch 7/15\n",
      " - 73s - loss: 0.0636\n",
      "Epoch 8/15\n",
      " - 73s - loss: 0.0628\n",
      "Epoch 9/15\n",
      " - 73s - loss: 0.0621\n",
      "Epoch 10/15\n",
      " - 72s - loss: 0.0614\n",
      "Epoch 11/15\n",
      " - 73s - loss: 0.0608\n",
      "Epoch 12/15\n",
      " - 73s - loss: 0.0602\n",
      "Epoch 13/15\n",
      " - 73s - loss: 0.0598\n",
      "Epoch 14/15\n",
      " - 73s - loss: 0.0593\n",
      "Epoch 15/15\n",
      " - 73s - loss: 0.0589\n",
      "Epoch 1/20\n",
      "1616/1616 [==============================] - 9s 6ms/step - loss: 0.0100\n",
      "Epoch 2/20\n",
      "1616/1616 [==============================] - 9s 6ms/step - loss: 0.0051\n",
      "Epoch 3/20\n",
      "1616/1616 [==============================] - 9s 6ms/step - loss: 0.0039\n",
      "Epoch 4/20\n",
      "1616/1616 [==============================] - 9s 6ms/step - loss: 0.0031\n",
      "Epoch 5/20\n",
      "1616/1616 [==============================] - 9s 6ms/step - loss: 0.0027\n",
      "Epoch 6/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0022\n",
      "Epoch 7/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0020\n",
      "Epoch 8/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0017\n",
      "Epoch 9/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0015\n",
      "Epoch 10/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0014\n",
      "Epoch 11/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0013\n",
      "Epoch 12/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0012\n",
      "Epoch 13/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0012\n",
      "Epoch 14/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0012\n",
      "Epoch 15/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0010\n",
      "Epoch 16/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 0.0010\n",
      "Epoch 17/20\n",
      "1616/1616 [==============================] - 9s 6ms/step - loss: 9.9415e-04\n",
      "Epoch 18/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 9.6455e-04\n",
      "Epoch 19/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 9.0200e-04\n",
      "Epoch 20/20\n",
      "1616/1616 [==============================] - 10s 6ms/step - loss: 8.8346e-04\n",
      "Attempt 1 finished.\n",
      "0.700869898528\n"
     ]
    }
   ],
   "source": [
    "max_pears = 0.64\n",
    "diff = 0.001\n",
    "\n",
    "for p in params_list:\n",
    "    average = 0.0\n",
    "    for i in range(1):\n",
    "        neural_model = create_model(p)\n",
    "        neural_model.fit(np.vstack((dirty_X)), \\\n",
    "                         np.vstack((dirty_Y)), \\\n",
    "                         epochs=p.dirty_e,\\\n",
    "                         batch_size=p.dirty_bs, verbose=2)\n",
    "        neural_model.fit(train_X, train_Y, epochs=p.train_e, batch_size=p.train_bs)\n",
    "        \n",
    "        print('Attempt %i finished.' % (i + 1))\n",
    "        \n",
    "        predictions = neural_model.predict(test_X)\n",
    "        preds = [pr[2] for pr in predictions]\n",
    "        results = [r[2] for r in test_Y]\n",
    "        \n",
    "        pears = pearsonr(results, preds)[0]\n",
    "        average += pears\n",
    "        print(pears)\n",
    "        \n",
    "        if pears >= max_pears + 0.001:\n",
    "            json_m = neural_model.to_json()\n",
    "            json_name = 'best_model_architecture_%s.json' % EMOTION\n",
    "            weights_name = 'best_model_weights_%s.h5' % EMOTION\n",
    "            with open('tokenizer_%s.pickle' % EMOTION, 'wb') as handle:\n",
    "                pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open(json_name, 'w') as outfile:\n",
    "                json.dump(json_m, outfile)\n",
    "            neural_model.save_weights(weights_name)\n",
    "            max_pears = pears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
