{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Считываем твиты, выделяем фичи\n",
    "\n",
    "1. Достаем чистые твиты\n",
    "2. Достаем development твиты\n",
    "3. Достаем грязные твиты\n",
    "4. Получаем фичи\n",
    "5. Сохраняем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import path, listdir\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
    "import resource\n",
    "from keras.models import Sequential, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GaussianNoise, SimpleRNN, LSTM, Reshape, Embedding, SpatialDropout1D, GaussianDropout, Conv1D, GlobalMaxPooling1D, Flatten, MaxPooling1D, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from collections import namedtuple\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import random as rn\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_from_emotion = {}\n",
    "position_from_emotion['anger'] = [1, 0, 0, 0]\n",
    "position_from_emotion['sadness'] = [0, 1, 0, 0]\n",
    "position_from_emotion['joy'] = [0, 0, 1, 0]\n",
    "position_from_emotion['fear'] = [0, 0, 0, 1]\n",
    "\n",
    "cl_from_emotion = {}\n",
    "cl_from_emotion['anger'] = 0\n",
    "cl_from_emotion['sadness'] = 1\n",
    "cl_from_emotion['joy'] = 2\n",
    "cl_from_emotion['fear'] = 3\n",
    "\n",
    "\n",
    "class Tweet(object):\n",
    "    def __init__(self, message, res, common_class):\n",
    "        self.cl = cl_from_emotion[common_class]\n",
    "        self.message = message\n",
    "        self.res = [x * res for x in position_from_emotion[common_class]]\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.message) + \" \" + str(self.res)\n",
    "\n",
    "\n",
    "def get_tweet(str_tweet, res_acc=1):\n",
    "    num, message, common_class, res = str_tweet.split('\\t')\n",
    "    if res == 'NONE':\n",
    "        res = '1.000'\n",
    "    return Tweet(message, float(res[0:res_acc]), common_class)\n",
    "        \n",
    "\n",
    "\n",
    "def get_tweets(str_tweets, res_acc=1):\n",
    "    return [get_tweet(line, res_acc) for line in str_tweets.split('\\n') if len(line) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation_f(y_true, y_pred):\n",
    "    fsp = y_pred - K.mean(y_pred,axis=-1,keepdims=True)\n",
    "    fst = y_true - K.mean(y_true,axis=-1, keepdims=True)\n",
    "\n",
    "    devP = K.std(y_pred)\n",
    "    devT = K.std(y_true)\n",
    "    \n",
    "    val = K.mean(fsp*fst)/(devP*devT)\n",
    "    \n",
    "    return 1 - val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARSE_SENT140 = False # True только если есть потребность поменять что-то \n",
    "\n",
    "def format_line(id, text, em):\n",
    "    return (str(id) + '\\t' + text.replace('\\t', ' ') + '\\t' + em + '\\t1.000\\n')\n",
    "\n",
    "sent_to_emotion = {0: 'sadness', 2: 'no_emotion', 4: 'joy'}\n",
    "\n",
    "if PARSE_SENT140:\n",
    "    data = pd.read_csv('dirty_data/unlabeled/tweet_corpus.csv', encoding = 'ISO-8859-1', index_col=0, parse_dates=True,\n",
    "                      names=['target', 'ids', 'date', 'flag', 'user', 'text'])\n",
    "    i  = 0\n",
    "    file = None\n",
    "    for id, row in data.iterrows():\n",
    "        if i % (data.shape[0] // 10) == 0:\n",
    "            part_number = str(i // (data.shape[0] // 10))\n",
    "            file = open('dirty_data/labeled/sent140part' + part_number, 'w+')\n",
    "        emotion = sent_to_emotion[id]\n",
    "        if emotion != 'no_emotion':\n",
    "            line = format_line(row[0], row[4], emotion)\n",
    "            file.write(line)\n",
    "        i += 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTIONS = ['anger', 'joy', 'sadness', 'fear']\n",
    "\n",
    "def run_competition_files(path_pattern):\n",
    "    em_tweets = {}\n",
    "    for emotion in EMOTIONS:\n",
    "        filename = path.join(path_pattern % emotion)\n",
    "        file = open(filename, 'r')\n",
    "        em_tweets[emotion] = get_tweets(file.read(), res_acc=5)\n",
    "        file.close()\n",
    "    return em_tweets\n",
    "    \n",
    "train_tweets = run_competition_files('train_data/EI-reg-en_%s_train.txt')\n",
    "test_tweets = run_competition_files('development_data/2018-EI-reg-En-%s-dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_tweets =[]\n",
    "\n",
    "directory = path.join('dirty_data/labeled')\n",
    "for filename in ['dirty_data.txt']:\n",
    "    file = open(path.join(directory,filename), 'r')\n",
    "    dirty_tweets += get_tweets(file.read(), res_acc=5)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION = 'joy'\n",
    "\n",
    "tweets = np.array(list(dirty_tweets) + list(train_tweets[EMOTION]) + list(test_tweets[EMOTION]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "    \n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def normalize_text(text):\n",
    "    stripped = re.sub(combined_pat, '', text)\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_TEXT = False\n",
    "\n",
    "dirty_texts, train_texts, test_texts, texts = [], [], [], []\n",
    "if not LOAD_TEXT:\n",
    "    texts = [normalize_text(t.message) for t in tweets]\n",
    "    dirty_texts = texts[0:len(dirty_tweets)]\n",
    "    train_texts = texts[len(dirty_tweets): len(dirty_tweets) + len(train_tweets[EMOTION])]\n",
    "    test_texts = texts[len(dirty_tweets) + len(train_tweets[EMOTION]):]\n",
    "    \n",
    "    assert (len(train_texts) == len(train_tweets[EMOTION]))\n",
    "    assert (len(test_texts) == len(test_tweets[EMOTION]))\n",
    "\n",
    "    np.savetxt('features/dirty_texts.txt', dirty_texts, fmt=\"%s\")\n",
    "    np.savetxt('features/train_texts_%s_.txt' % EMOTION, train_texts, fmt=\"%s\")\n",
    "    np.savetxt('features/test_texts_%s_.txt' % EMOTION, test_texts, fmt=\"%s\")\n",
    "else:\n",
    "    dirty_texts = list(np.loadtxt('features/dirty_texts.txt', dtype='str', delimiter='\\n'))\n",
    "    train_texts = list(np.loadtxt('features/train_texts_%s_.txt' % EMOTION, dtype='str', delimiter='\\n'))\n",
    "    test_texts = list(np.loadtxt('features/test_texts_%s_.txt' % EMOTION, dtype='str', delimiter='\\n'))\n",
    "    texts = dirty_texts + train_texts + test_texts\n",
    "    \n",
    "    assert (len(train_texts) == len(train_tweets[EMOTION]))\n",
    "    assert (len(test_texts) == len(test_tweets[EMOTION]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = len(dirty_tweets)\n",
    "f = s + len(train_tweets[EMOTION])\n",
    "e = f + len(test_tweets[EMOTION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_D2V = False\n",
    "\n",
    "Xd2v = {}\n",
    "\n",
    "if not LOAD_D2V:\n",
    "    model = Doc2Vec.load('doc2vec/doc2vec_model.doc2vec')\n",
    "    named_texts = [('dirty', dirty_texts), ('train', train_texts), ('test', test_texts)]\n",
    "    for name, text_list in named_texts:\n",
    "        Xd2v[name] = np.array([(model.infer_vector(x.split())) for x in text_list])\n",
    "        Xd2v[name] = Xd2v[name] - np.amin(Xd2v[name])\n",
    "        Xd2v[name] = Xd2v[name] / (np.linalg.norm(Xd2v[name]))\n",
    "    np.savetxt('features/dirty_Xd2v.txt', Xd2v['dirty']) \n",
    "    np.savetxt('features/train_Xd2v_%s.txt' % EMOTION, Xd2v['train'])\n",
    "    np.savetxt('features/dirty_Xd2v_%s.txt' % EMOTION, Xd2v['test'])\n",
    "else:\n",
    "    Xd2v['dirty'] = np.loadtxt('features/dirty_Xd2v.txt')\n",
    "    Xd2v['train'] = np.loadtxt('features/train_Xd2v_%s.txt' % EMOTION)\n",
    "    Xd2v['test'] = np.loadtxt('features/dirty_Xd2v_%s.txt' % EMOTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size = len(texts)\n",
    "\n",
    "#for i in range(10):\n",
    "#    texts += train_texts\n",
    "#for i in range(500):\n",
    "#    texts += test_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource.setrlimit(resource.RLIMIT_CORE, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.0005, max_features=2048)\n",
    "\n",
    "Xcv = vectorizer.fit_transform(texts).toarray()\n",
    "vectorizer = None\n",
    "\n",
    "texts = texts[0:old_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_Xd2v = np.vstack((Xd2v['dirty'],Xd2v['train'],Xd2v['test']))\n",
    "\n",
    "batch_size = len(dirty_tweets)\n",
    "\n",
    "dirty_X = np.hstack((Xcv[0:len(dirty_tweets)], all_Xd2v[0:len(dirty_tweets)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.hstack((Xcv[s:f], all_Xd2v[s:f]))\n",
    "test_X = np.hstack((Xcv[f:e], all_Xd2v[f:e]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (len(train_X) == len(train_tweets[EMOTION]))\n",
    "assert (len(test_X) == len(test_tweets[EMOTION]))\n",
    "\n",
    "Xcv = None\n",
    "Xd2v = None\n",
    "np.savetxt('features/train_X_%s_.txt' % EMOTION, train_X)\n",
    "np.savetxt('features/test_X_%s_.txt' % EMOTION, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "dirty_X = X[0:s]\n",
    "train_X = X[s:f]\n",
    "test_X = X[f:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([t.res for t in tweets])\n",
    "\n",
    "dirty_Y = Y[0:s]\n",
    "train_Y = Y[s:f]\n",
    "test_Y = Y[f:e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dirty_Y = np.concatenate((np.zeros((dirty_Y.shape[0], 4)),dirty_Y), axis=1)\n",
    "# train_Y = np.concatenate((train_Y, np.zeros((train_Y.shape[0], 4))), axis=1)\n",
    "# test_Y = np.concatenate((test_Y, np.zeros((test_Y.shape[0], 4))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = True\n",
    "\n",
    "if GPU:\n",
    "    num_GPU = 1\n",
    "    num_CPU = 1\n",
    "else:\n",
    "    num_CPU = 1\n",
    "    num_GPU = 0\n",
    "\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=4,\\\n",
    "        inter_op_parallelism_threads=4, allow_soft_placement=True,\\\n",
    "        device_count = {'CPU' : num_CPU, 'GPU' : num_GPU})\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_Y = np.hstack((np.ones((test_X.shape[0], 1)), np.zeros((test_X.shape[0], 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = namedtuple('Params', 'layers loss optimizer dirty_e dirty_bs train_e train_bs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_params(dirty_e, dirty_bs, train_e, train_bs, layers, optimizer='adam'):\n",
    "    return Params(layers, pearson_correlation_f, optimizer, dirty_e, dirty_bs, train_e, train_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    nm = Sequential()\n",
    "    for layer in params.layers:\n",
    "        nm.add(layer())\n",
    "    nm.compile(loss='mean_squared_error', optimizer=params.optimizer)\n",
    "    return nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 300\n",
    "embeddings = dict()\n",
    "embeddings = KeyedVectors.load_word2vec_format( \"twitter_sgns_subset.txt.gz\" , binary=False ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((max_features , embeddings_dim ) )\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    if index < max_features:\n",
    "        try: embedding_weights[index,:] = embeddings[word]\n",
    "        except: embedding_weights[index,:] = np.random.rand( 1 , embeddings_dim )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(27)\n",
    "\n",
    "params_list = []\n",
    "\n",
    "perceptron_layers = []\n",
    "perceptron_layers.append(lambda: Dense(1024, input_dim=train_X.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "perceptron_layers.append(lambda: Dense(256, kernel_initializer='uniform', activation='relu'))\n",
    "perceptron_layers.append(lambda: Dense(32, kernel_initializer='uniform', activation='relu'))\n",
    "perceptron_layers.append(lambda: Dense(8, kernel_initializer='uniform', activation='softmax'))\n",
    "\n",
    "lstm_layers = []\n",
    "lstm_layers.append(lambda: Embedding(max_features, embeddings_dim, input_length = 36, weights=[embedding_weights]))\n",
    "lstm_layers.append(lambda: Dropout(0.25))\n",
    "lstm_layers.append(lambda: Conv1D(embeddings_dim, 3, activation='relu', padding='valid', strides=1))\n",
    "lstm_layers.append(lambda: MaxPooling1D(pool_size=2))\n",
    "lstm_layers.append(lambda: LSTM(embeddings_dim, dropout=0.3, recurrent_dropout=0.3))\n",
    "lstm_layers.append(lambda: Dense(4, activation='sigmoid'))\n",
    "\n",
    "#params_list.append(create_params(8, 100, 10, 50, perceptron_layers))\n",
    "#params_list.append(create_params(15, 500, 10, 50, perceptron_layers))\n",
    "params_list.append(create_params(20, 1000, 15, 16, lstm_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " - 5s - loss: 0.1637\n",
      "Epoch 2/20\n",
      " - 3s - loss: 0.1436\n",
      "Epoch 3/20\n",
      " - 3s - loss: 0.1356\n",
      "Epoch 4/20\n",
      " - 3s - loss: 0.1292\n",
      "Epoch 5/20\n",
      " - 3s - loss: 0.1237\n",
      "Epoch 6/20\n",
      " - 3s - loss: 0.1184\n",
      "Epoch 7/20\n",
      " - 3s - loss: 0.1132\n",
      "Epoch 8/20\n",
      " - 3s - loss: 0.1080\n",
      "Epoch 9/20\n",
      " - 3s - loss: 0.1022\n",
      "Epoch 10/20\n",
      " - 3s - loss: 0.0977\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-64707eb9f9d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mneural_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mneural_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirty_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                          \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirty_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty_e\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty_bs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mneural_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_bs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_pears = 0.64\n",
    "diff = 0.001\n",
    "\n",
    "for p in params_list:\n",
    "    average = 0.0\n",
    "    for i in range(1):\n",
    "        neural_model = create_model(p)\n",
    "        neural_model.fit(np.vstack((dirty_X)), \\\n",
    "                         np.vstack((dirty_Y)), \\\n",
    "                         epochs=p.dirty_e,\\\n",
    "                         batch_size=p.dirty_bs, verbose=2)\n",
    "        neural_model.fit(train_X, train_Y, epochs=p.train_e, batch_size=p.train_bs, verbose=2)\n",
    "        \n",
    "        print('Attempt %i finished.' % (i + 1))\n",
    "        \n",
    "        predictions = neural_model.predict(test_X)\n",
    "        preds = [pr[0] for pr in predictions]\n",
    "        results = [r[0] for r in test_Y]\n",
    "        \n",
    "        neural_model.save('%s.h5' % EMOTION)\n",
    "        with open('tokenizer_%s.pickle' % EMOTION, 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        pears = pearsonr(results, preds)[0]\n",
    "        average += pears\n",
    "        print(pears)\n",
    "        if pears >= max_pears + 0.001:\n",
    "            json_m = neural_model.to_json()\n",
    "            with open('best_model_architecture.json', 'w') as outfile:\n",
    "                json.dump(json_m, outfile)\n",
    "            neural_model.save_weights('best_model_weights.h5')\n",
    "            max_pears = pears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "preds = [pr[2] for pr in predictions]\n",
    "results = [r[2] for r in test_Y]\n",
    "print(mean_squared_error(results, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max_pears > 0.64:\n",
    "    with open('best_model_architecture.json', 'r') as arch_file:\n",
    "        best_model = model_from_json(json.loads(arch_file.read()), \\\n",
    "                                     {'pearson_correlation_f' : pearson_correlation_f})\n",
    "        best_model.load_weights('best_model_weights.h5')\n",
    "        print(best_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
