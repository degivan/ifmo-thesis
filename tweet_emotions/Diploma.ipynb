{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mord import *\n",
    "from numpy import average\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "    def __init__(self, message, res):\n",
    "        self.message = message\n",
    "        self.res = res\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.message) + \" \" + str(self.res)\n",
    "\n",
    "\n",
    "def get_tweet(str_tweet, res_acc=1):\n",
    "    num, message, common_class, res = str_tweet.split('\\t')\n",
    "    return Tweet(message, float(res[0:res_acc]))\n",
    "\n",
    "\n",
    "def get_tweets(str_tweets, res_acc=1):\n",
    "    return [get_tweet(line, res_acc) for line in str_tweets.split('\\n') if len(line) > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY_char_ngrams(tweets, max_features=800):\n",
    "    vectorizer = CountVectorizer(analyzer='char_wb', max_features=max_features, ngram_range=(3, 8))\n",
    "    X = vectorizer.fit_transform([t.message for t in tweets]).toarray()\n",
    "    Y = [int(t.res) for t in tweets]\n",
    "    return X, Y\n",
    "\n",
    "def get_XY_word_ngrams(tweets, max_features=800):\n",
    "    vectorizer = CountVectorizer(max_features=max_features)\n",
    "    X = vectorizer.fit_transform([t.message for t in tweets]).toarray()\n",
    "    Y = [int(t.res) for t in tweets]\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "from features.nrc_lexicon import get_lexicon\n",
    "\n",
    "\n",
    "def count_caps(tweet):\n",
    "    caps = 0\n",
    "    for word in tweet.message.split():\n",
    "        caps += (len(word) > 2) & (word.isupper())\n",
    "    return caps\n",
    "\n",
    "\n",
    "def count_symbol(tweet, symbol):\n",
    "    return tweet.message.count(symbol)\n",
    "\n",
    "\n",
    "def starts_with_vowel(tweet):\n",
    "    return tweet.message[0] in 'AaIiEeUuOo'\n",
    "\n",
    "\n",
    "def count_intensity(tweet, emotion):\n",
    "    lex = get_lexicon()\n",
    "    intensity = 0.0\n",
    "    for word in tweet.message.split():\n",
    "        intensity += lex.get(emotion + '---' + word, 0.0)\n",
    "    return ceil(intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION = 'anger'\n",
    "FILENAME = '/home/vanyadeg/Desktop/diploma/data/EIoc/EI-oc-En-' + EMOTION + '-train.txt'\n",
    "\n",
    "file = open(FILENAME, 'r')\n",
    "tweets = get_tweets(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get additional tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADD_FILENAME = '/home/vanyadeg/dev/ifmo-software-design-hw/SELab2/new_tweets.txt'\n",
    "\n",
    "file = open(ADD_FILENAME, 'r')\n",
    "add_tweets = get_tweets(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(tweets):\n",
    "    for tweet in tweets:\n",
    "        tweet.message.replace('#', '')\n",
    "\n",
    "remove_hashtags(tweets)\n",
    "remove_hashtags(add_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(X, tweets, emotion):\n",
    "    X_list = X.tolist()\n",
    "    for x, tweet in zip(X_list, tweets):\n",
    "        x.append(count_caps(tweet))\n",
    "        x.append(count_symbol(tweet, '!'))\n",
    "        x.append(count_intensity(tweet, emotion))\n",
    "    return np.array(X_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side(y, border):\n",
    "    return int(y > border)\n",
    "\n",
    "\n",
    "class OrdinalClassifier(object):\n",
    "    def __init__(self, clf_supplier):\n",
    "        self.clf_supplier = clf_supplier\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        X, Y = list(X), list(Y)\n",
    "        self.classifiers = [self.get_classifier(X, Y, b) for b in [0, 1, 2]]\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = list(X)\n",
    "        return [self.predict_one(x) for x in X]\n",
    "    \n",
    "    \n",
    "    def predict_one(self, x):\n",
    "        results = [clf.predict_proba([x]).tolist()[0] for clf in self.classifiers]\n",
    "        probs = [1 - results[0][1]]\n",
    "        for idx in range(len(results) - 1):\n",
    "            probs.append(results[idx][1] - results[idx + 1][1])\n",
    "        probs.append(results[-1][1])\n",
    "        answer = probs.index(max(probs))\n",
    "        return answer\n",
    "    \n",
    "    \n",
    "    def get_classifier(self, X, Y, border):\n",
    "        border_y = np.array([get_side(y, border) for y in Y])\n",
    "        clf = self.clf_supplier()\n",
    "        clf.fit(X, border_y)\n",
    "        return clf\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Ordinal Classifier(' + str(self.clf_supplier()) +')'\n",
    "\n",
    "    \n",
    "def filter_index(X, index):\n",
    "    return [X[i] for i in index]\n",
    "\n",
    "\n",
    "\n",
    "def test_classifier(clf_supplier, train_X, train_Y, test_X, test_Y, addX, addY, metrics):\n",
    "    clf = clf_supplier()\n",
    "    trX = np.concatenate((np.array(train_X), np.array(addX)))\n",
    "    trY = np.concatenate((np.array(train_Y), np.array(addY)))\n",
    "    clf.fit(trX, trY)\n",
    "    predicted = clf.predict(np.array(test_X))\n",
    "    acc = accuracy_score(np.array(test_Y), predicted)\n",
    "    f1 = f1_score(np.array(test_Y), predicted, average='macro')\n",
    "    metrics.append((acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_suppliers(): \n",
    "    classifiers = []\n",
    "    regular_classifiers = [lambda: MultinomialNB(alpha=10**(-1))\n",
    "                     , lambda: AdaBoostClassifier()\n",
    "                     , lambda: DecisionTreeClassifier()]\n",
    "    classifiers += regular_classifiers\n",
    "    classifiers += [lambda suppl=clf_supplier: OrdinalClassifier(suppl) for clf_supplier in regular_classifiers]\n",
    "    ordinal_classifiers = [lambda: LogisticSE(max_iter=10 ** 6), \n",
    "                lambda: LogisticIT(max_iter=10 ** 6), \n",
    "                lambda: LogisticAT(max_iter=10 ** 6)]\n",
    "    classifiers += ordinal_classifiers\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)', \"AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=1.0, n_estimators=50, random_state=None)\", \"DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best')\", 'Ordinal Classifier(MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))', \"Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=1.0, n_estimators=50, random_state=None))\", \"Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best'))\", 'LogisticSE(alpha=1.0, max_iter=1000000, verbose=0)', 'LogisticIT(alpha=1.0, max_iter=1000000, verbose=0)', 'LogisticAT(alpha=1.0, max_iter=1000000, verbose=0)']\n",
      "1/9\n",
      "2/9\n",
      "3/9\n",
      "4/9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/9\n",
      "6/9\n",
      "7/9\n",
      "8/9\n",
      "9/9\n",
      "LogisticAT(alpha=1.0, max_iter=1000000, verbose=0) 0.284587356463\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) 0.379934269044\n",
      "LogisticSE(alpha=1.0, max_iter=1000000, verbose=0) 0.293484803793\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') 0.370733170416\n",
      "Ordinal Classifier(MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)) 0.224453047872\n",
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True) 0.256561416518\n",
      "Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)) 0.246829992094\n",
      "Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')) 0.332742981179\n",
      "LogisticIT(alpha=1.0, max_iter=1000000, verbose=0) 0.238683362677\n",
      "Max F: 500, func: get_XY_char_ngrams, best classifier: (\"AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=1.0, n_estimators=50, random_state=None)\", 0.37993426904412936)\n",
      "['MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)', \"AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=1.0, n_estimators=50, random_state=None)\", \"DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best')\", 'Ordinal Classifier(MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))', \"Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=1.0, n_estimators=50, random_state=None))\", \"Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best'))\", 'LogisticSE(alpha=1.0, max_iter=1000000, verbose=0)', 'LogisticIT(alpha=1.0, max_iter=1000000, verbose=0)', 'LogisticAT(alpha=1.0, max_iter=1000000, verbose=0)']\n",
      "1/9\n",
      "2/9\n",
      "3/9\n",
      "4/9\n",
      "5/9\n",
      "6/9\n",
      "7/9\n",
      "8/9\n",
      "9/9\n",
      "LogisticAT(alpha=1.0, max_iter=1000000, verbose=0) 0.291161577239\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None) 0.336267311204\n",
      "LogisticSE(alpha=1.0, max_iter=1000000, verbose=0) 0.291709647082\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') 0.384025501255\n",
      "Ordinal Classifier(MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)) 0.359012120355\n",
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True) 0.350054482419\n",
      "Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)) 0.257645670575\n",
      "Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')) 0.383608704698\n",
      "LogisticIT(alpha=1.0, max_iter=1000000, verbose=0) 0.23957905035\n",
      "Max F: 500, func: get_XY_word_ngrams, best classifier: (\"DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best')\", 0.38402550125547941)\n",
      "['MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)', \"AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=1.0, n_estimators=50, random_state=None)\", \"DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best')\", 'Ordinal Classifier(MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))', \"Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\\n          learning_rate=1.0, n_estimators=50, random_state=None))\", \"Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\\n            max_features=None, max_leaf_nodes=None,\\n            min_impurity_decrease=0.0, min_impurity_split=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\\n            splitter='best'))\", 'LogisticSE(alpha=1.0, max_iter=1000000, verbose=0)', 'LogisticIT(alpha=1.0, max_iter=1000000, verbose=0)', 'LogisticAT(alpha=1.0, max_iter=1000000, verbose=0)']\n",
      "1/9\n",
      "2/9\n",
      "3/9\n",
      "4/9\n"
     ]
    }
   ],
   "source": [
    "def fit_predict(X, Y, addX, addY):\n",
    "    f1_clf = {}\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    suppliers = get_clf_suppliers()\n",
    "    for idx, suppl in enumerate(suppliers):\n",
    "        metrics = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            train_X = filter_index(X, train_index)\n",
    "            train_Y = filter_index(Y, train_index)\n",
    "            test_X = filter_index(X, test_index)\n",
    "            test_Y = filter_index(Y, test_index)\n",
    "            test_classifier(suppl, train_X, train_Y, test_X, test_Y, addX, addY, metrics)\n",
    "        accuracies = [x[0] for x in metrics]\n",
    "        f1_scores = [x[1] for x in metrics]\n",
    "        f1_clf[str(suppl())] = average(f1_scores)\n",
    "        print \"{}/{}\".format(idx + 1, len(suppliers))\n",
    "    #for k, v in f1_clf.iteritems():\n",
    "    #    print k, v\n",
    "    best_clf = max(f1_clf, key=f1_clf.get)\n",
    "    return (best_clf, f1_clf[best_clf])\n",
    "\n",
    "RESULT_FORMAT = \"Max F: {}, func: {}, best classifier: {}\"\n",
    "\n",
    "for max_f in range(500, 1000, 100):\n",
    "    for feature_getter in [get_XY_char_ngrams, get_XY_word_ngrams]:\n",
    "        X, Y = feature_getter(tweets, max_features=max_f)\n",
    "        addX, addY = feature_getter(add_tweets, max_features=max_f)\n",
    "        for x in [X, addX]:\n",
    "            x = add_features(x, tweets, EMOTION)\n",
    "        print RESULT_FORMAT.format(max_f, feature_getter.func_name, fit_predict(X, Y, addX, addY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
