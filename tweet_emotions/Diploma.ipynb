{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mord import *\n",
    "from numpy import average\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet(object):\n",
    "    def __init__(self, message, res):\n",
    "        self.message = message\n",
    "        self.res = res\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.message) + \" \" + str(self.res)\n",
    "\n",
    "\n",
    "def get_tweet(str_tweet, res_acc=1):\n",
    "    num, message, common_class, res = str_tweet.split('\\t')\n",
    "    return Tweet(message, float(res[0:res_acc]))\n",
    "\n",
    "\n",
    "def get_tweets(str_tweets, res_acc=1):\n",
    "    return [get_tweet(line, res_acc) for line in str_tweets.split('\\n') if len(line) > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XY_char_ngrams(tweets):\n",
    "    vectorizer = CountVectorizer(analyzer='char_wb', max_features=500, ngram_range=(3, 8))\n",
    "    X = vectorizer.fit_transform([t.message for t in tweets]).toarray()\n",
    "    Y = [t.res for t in tweets]\n",
    "    return X, Y\n",
    "\n",
    "def get_XY_word_ngrams(tweets):\n",
    "    vectorizer = CountVectorizer(max_features=500)\n",
    "    X = vectorizer.fit_transform([t.message for t in tweets]).toarray()\n",
    "    Y = [t.res for t in tweets]\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "from features.nrc_lexicon import get_lexicon\n",
    "\n",
    "\n",
    "def count_caps(tweet):\n",
    "    caps = 0\n",
    "    for word in tweet.message.split():\n",
    "        caps += (len(word) > 2) & (word.isupper())\n",
    "    return caps\n",
    "\n",
    "\n",
    "def count_symbol(tweet, symbol):\n",
    "    return tweet.message.count(symbol)\n",
    "\n",
    "\n",
    "def starts_with_vowel(tweet):\n",
    "    return tweet.message[0] in 'AaIiEeUuOo'\n",
    "\n",
    "\n",
    "def count_intensity(tweet, emotion):\n",
    "    lex = get_lexicon()\n",
    "    intensity = 0.0\n",
    "    for word in tweet.message.split():\n",
    "        intensity += lex.get(emotion + '---' + word, 0.0)\n",
    "    return ceil(intensity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION = 'anger'\n",
    "FILENAME = '/home/vanyadeg/Desktop/diploma/data/EIoc/EI-oc-En-' + EMOTION + '-train.txt'\n",
    "\n",
    "file = open(FILENAME, 'r')\n",
    "tweets = get_tweets(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtags(tweets):\n",
    "    for tweet in tweets:\n",
    "        tweet.message.replace('#', '')\n",
    "\n",
    "remove_hashtags(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features(word n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(X, tweets, emotion):\n",
    "    X_list = X.tolist()\n",
    "    for x, tweet in zip(X_list, tweets):\n",
    "        x.append(count_caps(tweet))\n",
    "        x.append(count_symbol(tweet, '!'))\n",
    "        x.append(count_intensity(tweet, emotion))\n",
    "    return np.array(X_list)\n",
    "\n",
    "\n",
    "X, Y = get_XY_word_ngrams(tweets)\n",
    "X = add_features(X, tweets, EMOTION)\n",
    "X = VarianceThreshold().fit_transform(X)\n",
    "Y = [int(y) for y in Y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_side(y, border):\n",
    "    return int(y > border)\n",
    "\n",
    "\n",
    "class OrdinalClassifier(object):\n",
    "    def __init__(self, clf_supplier):\n",
    "        self.clf_supplier = clf_supplier\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        X, Y = list(X), list(Y)\n",
    "        self.classifiers = [self.get_classifier(X, Y, b) for b in [0, 1, 2]]\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = list(X)\n",
    "        return [self.predict_one(x) for x in X]\n",
    "    \n",
    "    \n",
    "    def predict_one(self, x):\n",
    "        results = [clf.predict_proba([x]).tolist()[0] for clf in self.classifiers]\n",
    "        probs = [1 - results[0][1]]\n",
    "        for idx in range(len(results) - 1):\n",
    "            probs.append(results[idx][1] - results[idx + 1][1])\n",
    "        probs.append(results[-1][1])\n",
    "        answer = probs.index(max(probs))\n",
    "        return answer\n",
    "    \n",
    "    \n",
    "    def get_classifier(self, X, Y, border):\n",
    "        border_y = [get_side(y, border) for y in Y]\n",
    "        clf = self.clf_supplier()\n",
    "        clf.fit(X, border_y)\n",
    "        return clf\n",
    "        \n",
    "    \n",
    "    def __str__(self):\n",
    "        return 'Ordinal Classifier(' + str(self.clf_supplier()) +')'\n",
    "\n",
    "    \n",
    "def filter_index(X, index):\n",
    "    return [X[i] for i in index]\n",
    "\n",
    "\n",
    "\n",
    "def test_classifier(clf, train_X, train_Y, test_X, test_Y, metrics):\n",
    "    clf.fit(np.array(train_X), np.array(train_Y))\n",
    "    predicted = clf.predict(np.array(test_X))\n",
    "    acc = accuracy_score(np.array(test_Y), predicted)\n",
    "    f1 = f1_score(np.array(test_Y), predicted, average='macro')\n",
    "    metrics.append((acc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classifiers(): \n",
    "    classifiers = []\n",
    "    for clf_supplier in [lambda: MultinomialNB(alpha=1.0)\n",
    "                     , lambda: AdaBoostClassifier()\n",
    "                     , lambda: DecisionTreeClassifier()]:\n",
    "        classifiers.append(clf_supplier())\n",
    "        classifiers.append(OrdinalClassifier(clf_supplier))\n",
    "    for clf in [LogisticSE(max_iter=10 ** 6), LogisticIT(max_iter=10 ** 6), LogisticAT(max_iter=10 ** 6)]:\n",
    "        classifiers.append(clf)\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True): 0.465056759546\n",
      "Average F1-score:MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True): 0.447756164061\n",
      "Average accuracy:Ordinal Classifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)): 0.467997936017\n",
      "Average F1-score:Ordinal Classifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)): 0.455379722972\n",
      "Average accuracy:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None): 0.430925352597\n",
      "Average F1-score:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None): 0.38255546006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)): 0.40385620915\n",
      "Average F1-score:Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)): 0.268101618359\n",
      "Average accuracy:DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'): 0.402091503268\n",
      "Average F1-score:DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'): 0.384342469846\n",
      "Average accuracy:Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')): 0.391544547644\n",
      "Average F1-score:Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')): 0.388900859275\n",
      "Average accuracy:LogisticSE(alpha=1.0, max_iter=1000000, verbose=0): 0.41091503268\n",
      "Average F1-score:LogisticSE(alpha=1.0, max_iter=1000000, verbose=0): 0.413575556366\n",
      "Average accuracy:LogisticIT(alpha=1.0, max_iter=1000000, verbose=0): 0.459697282422\n",
      "Average F1-score:LogisticIT(alpha=1.0, max_iter=1000000, verbose=0): 0.385627764281\n",
      "Average accuracy:LogisticAT(alpha=1.0, max_iter=1000000, verbose=0): 0.44916752666\n",
      "Average F1-score:LogisticAT(alpha=1.0, max_iter=1000000, verbose=0): 0.445646918639\n"
     ]
    }
   ],
   "source": [
    "def fit_predict(X, Y):\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    for clf in get_classifiers():\n",
    "        metrics = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            train_X = filter_index(X, train_index)\n",
    "            train_Y = filter_index(Y, train_index)\n",
    "            test_X = filter_index(X, test_index)\n",
    "            test_Y = filter_index(Y, test_index)\n",
    "            test_classifier(clf, train_X, train_Y, test_X, test_Y, metrics)\n",
    "        accuracies = [x[0] for x in metrics]\n",
    "        f1_scores = [x[1] for x in metrics]\n",
    "        print (\"Average accuracy:\" + str(clf) + \": \" + str(average(accuracies)))\n",
    "        print (\"Average F1-score:\" + str(clf) + \": \" + str(average(f1_scores)))\n",
    "        \n",
    "fit_predict(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features(char n-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_XY_char_ngrams(tweets)\n",
    "X = add_features(X, tweets, EMOTION)\n",
    "X = VarianceThreshold().fit_transform(X)\n",
    "Y = [int(y) for y in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy:MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True): 0.378582731338\n",
      "Average F1-score:MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True): 0.36756271347\n",
      "Average accuracy:Ordinal Classifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)): 0.389748882009\n",
      "Average F1-score:Ordinal Classifier(MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)): 0.37344277674\n",
      "Average accuracy:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None): 0.411517027864\n",
      "Average F1-score:AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None): 0.384787282027\n",
      "Average accuracy:Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)): 0.382707258342\n",
      "Average F1-score:Ordinal Classifier(AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
      "          learning_rate=1.0, n_estimators=50, random_state=None)): 0.25235942014\n",
      "Average accuracy:DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'): 0.364461644307\n",
      "Average F1-score:DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'): 0.352547815524\n",
      "Average accuracy:Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')): 0.361572067423\n",
      "Average F1-score:Ordinal Classifier(DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')): 0.365902569927\n",
      "Average accuracy:LogisticSE(alpha=1.0, max_iter=1000000, verbose=0): 0.364454764362\n",
      "Average F1-score:LogisticSE(alpha=1.0, max_iter=1000000, verbose=0): 0.367538580133\n",
      "Average accuracy:LogisticIT(alpha=1.0, max_iter=1000000, verbose=0): 0.418575851393\n",
      "Average F1-score:LogisticIT(alpha=1.0, max_iter=1000000, verbose=0): 0.35136695116\n",
      "Average accuracy:LogisticAT(alpha=1.0, max_iter=1000000, verbose=0): 0.385053319573\n",
      "Average F1-score:LogisticAT(alpha=1.0, max_iter=1000000, verbose=0): 0.384722645624\n"
     ]
    }
   ],
   "source": [
    "fit_predict(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
